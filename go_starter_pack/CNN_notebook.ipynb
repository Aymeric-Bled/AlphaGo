{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcazSkjzqIWH",
        "outputId": "1cc2ff13-0f95-4a5b-a78d-8ad33e7a5627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsdCMI457kdK"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import randint\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import gdrive.MyDrive.AlphaGo.Goban as Goban\n",
        "import torch.optim as optim\n",
        "from gdrive.MyDrive.AlphaGo.Goban import Board"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device.type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v44xwGsusgLL",
        "outputId": "41ee1c76-f30c-4ead-8183-38d21166c8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gdrive/MyDrive/AlphaGo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99JK4IcNrc1W",
        "outputId": "91552577-ad2a-4095-c390-931fb14f9067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN.ipynb  Goban.py  model_weights2.pth  model_weights.pth  __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1apOGNSt8ACZ"
      },
      "outputs": [],
      "source": [
        "def get_raw_data_go():\n",
        "    ''' Returns the set of samples from the local file or download it if it does not exists'''\n",
        "\n",
        "    raw_samples_file = \"samples-9x9.json.gz\"\n",
        "\n",
        "    if not os.path.isfile(raw_samples_file):\n",
        "        print(\"File\", raw_samples_file, \"not found, I am downloading it...\", end=\"\")\n",
        "        urllib.request.urlretrieve(\"https://www.labri.fr/perso/lsimon/ia-inge2/samples-9x9.json.gz\", \"samples-9x9.json.gz\")\n",
        "        print(\" Done\")\n",
        "\n",
        "    with gzip.open(\"samples-9x9.json.gz\") as fz:\n",
        "        data = json.loads(fz.read().decode(\"utf-8\"))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsqVK-q8Ckr"
      },
      "outputs": [],
      "source": [
        "class ConvolutionBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvolutionBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(9, 9,kernel_size=5, padding='same')\n",
        "        self.bn = nn.BatchNorm2d(9)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        conv = self.conv(x)\n",
        "        conv += x\n",
        "        x1 = self.bn(conv)\n",
        "        return self.relu(x1)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.res1 = ConvolutionBlock()\n",
        "        self.res2 = ConvolutionBlock()\n",
        "        self.res3 = ConvolutionBlock()\n",
        "        self.res4 = ConvolutionBlock()\n",
        "        self.res5 = ConvolutionBlock()\n",
        "        self.linear1 = nn.Linear(9 * 81 * 2, 82)\n",
        "        self.linear2 = nn.Linear(9 * 81 * 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax(0)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.res5(x)\n",
        "        x = torch.flatten(x)\n",
        "        p = self.softmax(self.dropout1(self.linear1(x.clone())))\n",
        "        v = self.sigmoid(self.dropout2(self.linear2(x.clone())))\n",
        "        \n",
        "        return (p,v)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g7CfoDg8Ggo"
      },
      "outputs": [],
      "source": [
        "def name_to_coord(s):\n",
        "    assert s != \"PASS\"\n",
        "    indexLetters = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'J':8}\n",
        "\n",
        "    col = indexLetters[s[0]]\n",
        "    lin = int(s[1:]) - 1\n",
        "    return col, lin\n",
        "\n",
        "def rotation(board):\n",
        "    L = []\n",
        "    L.append(np.reshape(board, 81))\n",
        "    for i in range(1, 4):\n",
        "        rotation = np.rot90(board, i)\n",
        "        L.append(np.reshape(rotation, 81))\n",
        "    return L\n",
        "\n",
        "def symmetry(board):\n",
        "    L = rotation(np.reshape(board, (9,9))) + rotation(np.transpose(np.reshape(board, (9,9))))\n",
        "    return L\n",
        "\n",
        "def coord_to_board(black_moves, white_moves):\n",
        "    board = [[0. for i in range(9)] for j in range(9)]\n",
        "    for x,y in black_moves:\n",
        "        board[x][y] = 1.\n",
        "    for x,y in white_moves:\n",
        "        board[x][y] = -1.\n",
        "    return board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEK8NBHW8JQt"
      },
      "outputs": [],
      "source": [
        "def get_data(data):\n",
        "    D = []\n",
        "    j = 0\n",
        "    length = len(data)\n",
        "    for game in data:\n",
        "        print(j, \"/\", length)\n",
        "        j += 1\n",
        "        if j == 1000:\n",
        "            break\n",
        "        if game['depth'] < 9:\n",
        "            continue\n",
        "        board_black_list = []\n",
        "        board_white_list = []\n",
        "        board = Goban.Board()\n",
        "        err = False\n",
        "        for m in game['list_of_moves']:\n",
        "            if m == 'PASS':\n",
        "                move = -1\n",
        "            else:\n",
        "                (x,y) = name_to_coord(m)\n",
        "                move = y * board._BOARDSIZE + x\n",
        "            try:\n",
        "                board.push(move)\n",
        "            except:\n",
        "                err = True\n",
        "                break\n",
        "            \n",
        "            board_black = board._board.copy()\n",
        "            board_white = board._board.copy()\n",
        "            for i in range(board._BOARDSIZE ** 2):\n",
        "                if board_black[i] == 2:\n",
        "                    board_black[i] = 0\n",
        "                if board_white[i] == 1:\n",
        "                    board_white[i] = 0\n",
        "                if board_white[i] == 2:\n",
        "                    board_white[i] = 1 \n",
        "            board_black_list.append(np.array(board_black))\n",
        "            board_white_list.append(np.array(board_white))\n",
        "        if err:\n",
        "            continue\n",
        "        hist_black = board_black_list[-9:-1]\n",
        "        hist_white = board_black_list[-9:-1]\n",
        "        hist_black_s = [symmetry(board) for board in hist_black]\n",
        "        hist_white_s = [symmetry(board) for board in hist_white]\n",
        "        for sym in range(8):\n",
        "            hist = [np.array([hist_black_s[i][sym], hist_white_s[i][sym]]) for i in range(8)]\n",
        "            v = None\n",
        "            if (game['depth'] - 1) %2 == 0:\n",
        "                hist.append(np.array([np.ones(board._BOARDSIZE ** 2, dtype=np.uint8), np.ones(board._BOARDSIZE ** 2, dtype=np.uint8)])) #BLACK to 1\n",
        "                v = int(game['black_wins']) / 100\n",
        "            else:\n",
        "                hist.append(np.array([np.zeros(board._BOARDSIZE ** 2, dtype=np.uint8), np.zeros(board._BOARDSIZE ** 2, dtype=np.uint8)])) #WHITE to 0\n",
        "                v = int(game['white_wins']) / 100\n",
        "            hist = np.array(hist) \n",
        "            m = game['list_of_moves'][-1]\n",
        "            if m == 'PASS':\n",
        "                move = board._BOARDSIZE ** 2\n",
        "            else:\n",
        "                (x,y) = name_to_coord(m)\n",
        "                move = y * board._BOARDSIZE + x\n",
        "            p = np.zeros(board._BOARDSIZE ** 2 + 1, dtype=np.uint8)\n",
        "            p[move] = 1\n",
        "\n",
        "            d = torch.reshape(torch.from_numpy(hist), (1,9,2,81)).type(torch.float32)\n",
        "            D.append((d.to(device),torch.tensor(p, dtype=torch.float32).to(device),torch.tensor(v, dtype=torch.float32).to(device)))\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04uVEtcj8N_D",
        "outputId": "80838557-8966-4ebe-b4a2-bfcc0664e14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File samples-9x9.json.gz not found, I am downloading it... Done\n",
            "0 / 41563\n",
            "1 / 41563\n",
            "2 / 41563\n",
            "3 / 41563\n",
            "4 / 41563\n",
            "5 / 41563\n",
            "6 / 41563\n",
            "7 / 41563\n",
            "8 / 41563\n",
            "9 / 41563\n",
            "10 / 41563\n",
            "11 / 41563\n",
            "12 / 41563\n",
            "13 / 41563\n",
            "14 / 41563\n",
            "15 / 41563\n",
            "16 / 41563\n",
            "17 / 41563\n",
            "18 / 41563\n",
            "19 / 41563\n",
            "20 / 41563\n",
            "21 / 41563\n",
            "22 / 41563\n",
            "23 / 41563\n",
            "24 / 41563\n",
            "25 / 41563\n",
            "26 / 41563\n",
            "27 / 41563\n",
            "28 / 41563\n",
            "29 / 41563\n",
            "30 / 41563\n",
            "31 / 41563\n",
            "32 / 41563\n",
            "33 / 41563\n",
            "34 / 41563\n",
            "35 / 41563\n",
            "36 / 41563\n",
            "37 / 41563\n",
            "38 / 41563\n",
            "39 / 41563\n",
            "40 / 41563\n",
            "41 / 41563\n",
            "42 / 41563\n",
            "43 / 41563\n",
            "44 / 41563\n",
            "45 / 41563\n",
            "46 / 41563\n",
            "47 / 41563\n",
            "48 / 41563\n",
            "49 / 41563\n",
            "50 / 41563\n",
            "51 / 41563\n",
            "52 / 41563\n",
            "53 / 41563\n",
            "54 / 41563\n",
            "55 / 41563\n",
            "56 / 41563\n",
            "57 / 41563\n",
            "58 / 41563\n",
            "59 / 41563\n",
            "60 / 41563\n",
            "61 / 41563\n",
            "62 / 41563\n",
            "63 / 41563\n",
            "64 / 41563\n",
            "65 / 41563\n",
            "66 / 41563\n",
            "67 / 41563\n",
            "68 / 41563\n",
            "69 / 41563\n",
            "70 / 41563\n",
            "71 / 41563\n",
            "72 / 41563\n",
            "73 / 41563\n",
            "74 / 41563\n",
            "75 / 41563\n",
            "76 / 41563\n",
            "77 / 41563\n",
            "78 / 41563\n",
            "79 / 41563\n",
            "80 / 41563\n",
            "81 / 41563\n",
            "82 / 41563\n",
            "83 / 41563\n",
            "84 / 41563\n",
            "85 / 41563\n",
            "86 / 41563\n",
            "87 / 41563\n",
            "88 / 41563\n",
            "89 / 41563\n",
            "90 / 41563\n",
            "91 / 41563\n",
            "92 / 41563\n",
            "93 / 41563\n",
            "94 / 41563\n",
            "95 / 41563\n",
            "96 / 41563\n",
            "97 / 41563\n",
            "98 / 41563\n",
            "99 / 41563\n",
            "100 / 41563\n",
            "101 / 41563\n",
            "102 / 41563\n",
            "103 / 41563\n",
            "104 / 41563\n",
            "105 / 41563\n",
            "106 / 41563\n",
            "107 / 41563\n",
            "108 / 41563\n",
            "109 / 41563\n",
            "110 / 41563\n",
            "111 / 41563\n",
            "112 / 41563\n",
            "113 / 41563\n",
            "114 / 41563\n",
            "115 / 41563\n",
            "116 / 41563\n",
            "117 / 41563\n",
            "118 / 41563\n",
            "119 / 41563\n",
            "120 / 41563\n",
            "121 / 41563\n",
            "122 / 41563\n",
            "123 / 41563\n",
            "124 / 41563\n",
            "125 / 41563\n",
            "126 / 41563\n",
            "127 / 41563\n",
            "128 / 41563\n",
            "129 / 41563\n",
            "130 / 41563\n",
            "131 / 41563\n",
            "132 / 41563\n",
            "133 / 41563\n",
            "134 / 41563\n",
            "135 / 41563\n",
            "136 / 41563\n",
            "137 / 41563\n",
            "138 / 41563\n",
            "139 / 41563\n",
            "140 / 41563\n",
            "141 / 41563\n",
            "142 / 41563\n",
            "143 / 41563\n",
            "144 / 41563\n",
            "145 / 41563\n",
            "146 / 41563\n",
            "147 / 41563\n",
            "148 / 41563\n",
            "149 / 41563\n",
            "150 / 41563\n",
            "151 / 41563\n",
            "152 / 41563\n",
            "153 / 41563\n",
            "154 / 41563\n",
            "155 / 41563\n",
            "156 / 41563\n",
            "157 / 41563\n",
            "158 / 41563\n",
            "159 / 41563\n",
            "160 / 41563\n",
            "161 / 41563\n",
            "162 / 41563\n",
            "163 / 41563\n",
            "164 / 41563\n",
            "165 / 41563\n",
            "166 / 41563\n",
            "167 / 41563\n",
            "168 / 41563\n",
            "169 / 41563\n",
            "170 / 41563\n",
            "171 / 41563\n",
            "172 / 41563\n",
            "173 / 41563\n",
            "174 / 41563\n",
            "175 / 41563\n",
            "176 / 41563\n",
            "177 / 41563\n",
            "178 / 41563\n",
            "179 / 41563\n",
            "180 / 41563\n",
            "181 / 41563\n",
            "182 / 41563\n",
            "183 / 41563\n",
            "184 / 41563\n",
            "185 / 41563\n",
            "186 / 41563\n",
            "187 / 41563\n",
            "188 / 41563\n",
            "189 / 41563\n",
            "190 / 41563\n",
            "191 / 41563\n",
            "192 / 41563\n",
            "193 / 41563\n",
            "194 / 41563\n",
            "195 / 41563\n",
            "196 / 41563\n",
            "197 / 41563\n",
            "198 / 41563\n",
            "199 / 41563\n",
            "200 / 41563\n",
            "201 / 41563\n",
            "202 / 41563\n",
            "203 / 41563\n",
            "204 / 41563\n",
            "205 / 41563\n",
            "206 / 41563\n",
            "207 / 41563\n",
            "208 / 41563\n",
            "209 / 41563\n",
            "210 / 41563\n",
            "211 / 41563\n",
            "212 / 41563\n",
            "213 / 41563\n",
            "214 / 41563\n",
            "215 / 41563\n",
            "216 / 41563\n",
            "217 / 41563\n",
            "218 / 41563\n",
            "219 / 41563\n",
            "220 / 41563\n",
            "221 / 41563\n",
            "222 / 41563\n",
            "223 / 41563\n",
            "224 / 41563\n",
            "225 / 41563\n",
            "226 / 41563\n",
            "227 / 41563\n",
            "228 / 41563\n",
            "229 / 41563\n",
            "230 / 41563\n",
            "231 / 41563\n",
            "232 / 41563\n",
            "233 / 41563\n",
            "234 / 41563\n",
            "235 / 41563\n",
            "236 / 41563\n",
            "237 / 41563\n",
            "238 / 41563\n",
            "239 / 41563\n",
            "240 / 41563\n",
            "241 / 41563\n",
            "242 / 41563\n",
            "243 / 41563\n",
            "244 / 41563\n",
            "245 / 41563\n",
            "246 / 41563\n",
            "247 / 41563\n",
            "248 / 41563\n",
            "249 / 41563\n",
            "250 / 41563\n",
            "251 / 41563\n",
            "252 / 41563\n",
            "253 / 41563\n",
            "254 / 41563\n",
            "255 / 41563\n",
            "256 / 41563\n",
            "257 / 41563\n",
            "258 / 41563\n",
            "259 / 41563\n",
            "260 / 41563\n",
            "261 / 41563\n",
            "262 / 41563\n",
            "263 / 41563\n",
            "264 / 41563\n",
            "265 / 41563\n",
            "266 / 41563\n",
            "267 / 41563\n",
            "268 / 41563\n",
            "269 / 41563\n",
            "270 / 41563\n",
            "271 / 41563\n",
            "272 / 41563\n",
            "273 / 41563\n",
            "274 / 41563\n",
            "275 / 41563\n",
            "276 / 41563\n",
            "277 / 41563\n",
            "278 / 41563\n",
            "279 / 41563\n",
            "280 / 41563\n",
            "281 / 41563\n",
            "282 / 41563\n",
            "283 / 41563\n",
            "284 / 41563\n",
            "285 / 41563\n",
            "286 / 41563\n",
            "287 / 41563\n",
            "288 / 41563\n",
            "289 / 41563\n",
            "290 / 41563\n",
            "291 / 41563\n",
            "292 / 41563\n",
            "293 / 41563\n",
            "294 / 41563\n",
            "295 / 41563\n",
            "296 / 41563\n",
            "297 / 41563\n",
            "298 / 41563\n",
            "299 / 41563\n",
            "300 / 41563\n",
            "301 / 41563\n",
            "302 / 41563\n",
            "303 / 41563\n",
            "304 / 41563\n",
            "305 / 41563\n",
            "306 / 41563\n",
            "307 / 41563\n",
            "308 / 41563\n",
            "309 / 41563\n",
            "310 / 41563\n",
            "311 / 41563\n",
            "312 / 41563\n",
            "313 / 41563\n",
            "314 / 41563\n",
            "315 / 41563\n",
            "316 / 41563\n",
            "317 / 41563\n",
            "318 / 41563\n",
            "319 / 41563\n",
            "320 / 41563\n",
            "321 / 41563\n",
            "322 / 41563\n",
            "323 / 41563\n",
            "324 / 41563\n",
            "325 / 41563\n",
            "326 / 41563\n",
            "327 / 41563\n",
            "328 / 41563\n",
            "329 / 41563\n",
            "330 / 41563\n",
            "331 / 41563\n",
            "332 / 41563\n",
            "333 / 41563\n",
            "334 / 41563\n",
            "335 / 41563\n",
            "336 / 41563\n",
            "337 / 41563\n",
            "338 / 41563\n",
            "339 / 41563\n",
            "340 / 41563\n",
            "341 / 41563\n",
            "342 / 41563\n",
            "343 / 41563\n",
            "344 / 41563\n",
            "345 / 41563\n",
            "346 / 41563\n",
            "347 / 41563\n",
            "348 / 41563\n",
            "349 / 41563\n",
            "350 / 41563\n",
            "351 / 41563\n",
            "352 / 41563\n",
            "353 / 41563\n",
            "354 / 41563\n",
            "355 / 41563\n",
            "356 / 41563\n",
            "357 / 41563\n",
            "358 / 41563\n",
            "359 / 41563\n",
            "360 / 41563\n",
            "361 / 41563\n",
            "362 / 41563\n",
            "363 / 41563\n",
            "364 / 41563\n",
            "365 / 41563\n",
            "366 / 41563\n",
            "367 / 41563\n",
            "368 / 41563\n",
            "369 / 41563\n",
            "370 / 41563\n",
            "371 / 41563\n",
            "372 / 41563\n",
            "373 / 41563\n",
            "374 / 41563\n",
            "375 / 41563\n",
            "376 / 41563\n",
            "377 / 41563\n",
            "378 / 41563\n",
            "379 / 41563\n",
            "380 / 41563\n",
            "381 / 41563\n",
            "382 / 41563\n",
            "383 / 41563\n",
            "384 / 41563\n",
            "385 / 41563\n",
            "386 / 41563\n",
            "387 / 41563\n",
            "388 / 41563\n",
            "389 / 41563\n",
            "390 / 41563\n",
            "391 / 41563\n",
            "392 / 41563\n",
            "393 / 41563\n",
            "394 / 41563\n",
            "395 / 41563\n",
            "396 / 41563\n",
            "397 / 41563\n",
            "398 / 41563\n",
            "399 / 41563\n",
            "400 / 41563\n",
            "401 / 41563\n",
            "402 / 41563\n",
            "403 / 41563\n",
            "404 / 41563\n",
            "405 / 41563\n",
            "406 / 41563\n",
            "407 / 41563\n",
            "408 / 41563\n",
            "409 / 41563\n",
            "410 / 41563\n",
            "411 / 41563\n",
            "412 / 41563\n",
            "413 / 41563\n",
            "414 / 41563\n",
            "415 / 41563\n",
            "416 / 41563\n",
            "417 / 41563\n",
            "418 / 41563\n",
            "419 / 41563\n",
            "420 / 41563\n",
            "421 / 41563\n",
            "422 / 41563\n",
            "423 / 41563\n",
            "424 / 41563\n",
            "425 / 41563\n",
            "426 / 41563\n",
            "427 / 41563\n",
            "428 / 41563\n",
            "429 / 41563\n",
            "430 / 41563\n",
            "431 / 41563\n",
            "432 / 41563\n",
            "433 / 41563\n",
            "434 / 41563\n",
            "435 / 41563\n",
            "436 / 41563\n",
            "437 / 41563\n",
            "438 / 41563\n",
            "439 / 41563\n",
            "440 / 41563\n",
            "441 / 41563\n",
            "442 / 41563\n",
            "443 / 41563\n",
            "444 / 41563\n",
            "445 / 41563\n",
            "446 / 41563\n",
            "447 / 41563\n",
            "448 / 41563\n",
            "449 / 41563\n",
            "450 / 41563\n",
            "451 / 41563\n",
            "452 / 41563\n",
            "453 / 41563\n",
            "454 / 41563\n",
            "455 / 41563\n",
            "456 / 41563\n",
            "457 / 41563\n",
            "458 / 41563\n",
            "459 / 41563\n",
            "460 / 41563\n",
            "461 / 41563\n",
            "462 / 41563\n",
            "463 / 41563\n",
            "464 / 41563\n",
            "465 / 41563\n",
            "466 / 41563\n",
            "467 / 41563\n",
            "468 / 41563\n",
            "469 / 41563\n",
            "470 / 41563\n",
            "471 / 41563\n",
            "472 / 41563\n",
            "473 / 41563\n",
            "474 / 41563\n",
            "475 / 41563\n",
            "476 / 41563\n",
            "477 / 41563\n",
            "478 / 41563\n",
            "479 / 41563\n",
            "480 / 41563\n",
            "481 / 41563\n",
            "482 / 41563\n",
            "483 / 41563\n",
            "484 / 41563\n",
            "485 / 41563\n",
            "486 / 41563\n",
            "487 / 41563\n",
            "488 / 41563\n",
            "489 / 41563\n",
            "490 / 41563\n",
            "491 / 41563\n",
            "492 / 41563\n",
            "493 / 41563\n",
            "494 / 41563\n",
            "495 / 41563\n",
            "496 / 41563\n",
            "497 / 41563\n",
            "498 / 41563\n",
            "499 / 41563\n",
            "500 / 41563\n",
            "501 / 41563\n",
            "502 / 41563\n",
            "503 / 41563\n",
            "504 / 41563\n",
            "505 / 41563\n",
            "506 / 41563\n",
            "507 / 41563\n",
            "508 / 41563\n",
            "509 / 41563\n",
            "510 / 41563\n",
            "511 / 41563\n",
            "512 / 41563\n",
            "513 / 41563\n",
            "514 / 41563\n",
            "515 / 41563\n",
            "516 / 41563\n",
            "517 / 41563\n",
            "518 / 41563\n",
            "519 / 41563\n",
            "520 / 41563\n",
            "521 / 41563\n",
            "522 / 41563\n",
            "523 / 41563\n",
            "524 / 41563\n",
            "525 / 41563\n",
            "526 / 41563\n",
            "527 / 41563\n",
            "528 / 41563\n",
            "529 / 41563\n",
            "530 / 41563\n",
            "531 / 41563\n",
            "532 / 41563\n",
            "533 / 41563\n",
            "534 / 41563\n",
            "535 / 41563\n",
            "536 / 41563\n",
            "537 / 41563\n",
            "538 / 41563\n",
            "539 / 41563\n",
            "540 / 41563\n",
            "541 / 41563\n",
            "542 / 41563\n",
            "543 / 41563\n",
            "544 / 41563\n",
            "545 / 41563\n",
            "546 / 41563\n",
            "547 / 41563\n",
            "548 / 41563\n",
            "549 / 41563\n",
            "550 / 41563\n",
            "551 / 41563\n",
            "552 / 41563\n",
            "553 / 41563\n",
            "554 / 41563\n",
            "555 / 41563\n",
            "556 / 41563\n",
            "557 / 41563\n",
            "558 / 41563\n",
            "559 / 41563\n",
            "560 / 41563\n",
            "561 / 41563\n",
            "562 / 41563\n",
            "563 / 41563\n",
            "564 / 41563\n",
            "565 / 41563\n",
            "566 / 41563\n",
            "567 / 41563\n",
            "568 / 41563\n",
            "569 / 41563\n",
            "570 / 41563\n",
            "571 / 41563\n",
            "572 / 41563\n",
            "573 / 41563\n",
            "574 / 41563\n",
            "575 / 41563\n",
            "576 / 41563\n",
            "577 / 41563\n",
            "578 / 41563\n",
            "579 / 41563\n",
            "580 / 41563\n",
            "581 / 41563\n",
            "582 / 41563\n",
            "583 / 41563\n",
            "584 / 41563\n",
            "585 / 41563\n",
            "586 / 41563\n",
            "587 / 41563\n",
            "588 / 41563\n",
            "589 / 41563\n",
            "590 / 41563\n",
            "591 / 41563\n",
            "592 / 41563\n",
            "593 / 41563\n",
            "594 / 41563\n",
            "595 / 41563\n",
            "596 / 41563\n",
            "597 / 41563\n",
            "598 / 41563\n",
            "599 / 41563\n",
            "600 / 41563\n",
            "601 / 41563\n",
            "602 / 41563\n",
            "603 / 41563\n",
            "604 / 41563\n",
            "605 / 41563\n",
            "606 / 41563\n",
            "607 / 41563\n",
            "608 / 41563\n",
            "609 / 41563\n",
            "610 / 41563\n",
            "611 / 41563\n",
            "612 / 41563\n",
            "613 / 41563\n",
            "614 / 41563\n",
            "615 / 41563\n",
            "616 / 41563\n",
            "617 / 41563\n",
            "618 / 41563\n",
            "619 / 41563\n",
            "620 / 41563\n",
            "621 / 41563\n",
            "622 / 41563\n",
            "623 / 41563\n",
            "624 / 41563\n",
            "625 / 41563\n",
            "626 / 41563\n",
            "627 / 41563\n",
            "628 / 41563\n",
            "629 / 41563\n",
            "630 / 41563\n",
            "631 / 41563\n",
            "632 / 41563\n",
            "633 / 41563\n",
            "634 / 41563\n",
            "635 / 41563\n",
            "636 / 41563\n",
            "637 / 41563\n",
            "638 / 41563\n",
            "639 / 41563\n",
            "640 / 41563\n",
            "641 / 41563\n",
            "642 / 41563\n",
            "643 / 41563\n",
            "644 / 41563\n",
            "645 / 41563\n",
            "646 / 41563\n",
            "647 / 41563\n",
            "648 / 41563\n",
            "649 / 41563\n",
            "650 / 41563\n",
            "651 / 41563\n",
            "652 / 41563\n",
            "653 / 41563\n",
            "654 / 41563\n",
            "655 / 41563\n",
            "656 / 41563\n",
            "657 / 41563\n",
            "658 / 41563\n",
            "659 / 41563\n",
            "660 / 41563\n",
            "661 / 41563\n",
            "662 / 41563\n",
            "663 / 41563\n",
            "664 / 41563\n",
            "665 / 41563\n",
            "666 / 41563\n",
            "667 / 41563\n",
            "668 / 41563\n",
            "669 / 41563\n",
            "670 / 41563\n",
            "671 / 41563\n",
            "672 / 41563\n",
            "673 / 41563\n",
            "674 / 41563\n",
            "675 / 41563\n",
            "676 / 41563\n",
            "677 / 41563\n",
            "678 / 41563\n",
            "679 / 41563\n",
            "680 / 41563\n",
            "681 / 41563\n",
            "682 / 41563\n",
            "683 / 41563\n",
            "684 / 41563\n",
            "685 / 41563\n",
            "686 / 41563\n",
            "687 / 41563\n",
            "688 / 41563\n",
            "689 / 41563\n",
            "690 / 41563\n",
            "691 / 41563\n",
            "692 / 41563\n",
            "693 / 41563\n",
            "694 / 41563\n",
            "695 / 41563\n",
            "696 / 41563\n",
            "697 / 41563\n",
            "698 / 41563\n",
            "699 / 41563\n",
            "700 / 41563\n",
            "701 / 41563\n",
            "702 / 41563\n",
            "703 / 41563\n",
            "704 / 41563\n",
            "705 / 41563\n",
            "706 / 41563\n",
            "707 / 41563\n",
            "708 / 41563\n",
            "709 / 41563\n",
            "710 / 41563\n",
            "711 / 41563\n",
            "712 / 41563\n",
            "713 / 41563\n",
            "714 / 41563\n",
            "715 / 41563\n",
            "716 / 41563\n",
            "717 / 41563\n",
            "718 / 41563\n",
            "719 / 41563\n",
            "720 / 41563\n",
            "721 / 41563\n",
            "722 / 41563\n",
            "723 / 41563\n",
            "724 / 41563\n",
            "725 / 41563\n",
            "726 / 41563\n",
            "727 / 41563\n",
            "728 / 41563\n",
            "729 / 41563\n",
            "730 / 41563\n",
            "731 / 41563\n",
            "732 / 41563\n",
            "733 / 41563\n",
            "734 / 41563\n",
            "735 / 41563\n",
            "736 / 41563\n",
            "737 / 41563\n",
            "738 / 41563\n",
            "739 / 41563\n",
            "740 / 41563\n",
            "741 / 41563\n",
            "742 / 41563\n",
            "743 / 41563\n",
            "744 / 41563\n",
            "745 / 41563\n",
            "746 / 41563\n",
            "747 / 41563\n",
            "748 / 41563\n",
            "749 / 41563\n",
            "750 / 41563\n",
            "751 / 41563\n",
            "752 / 41563\n",
            "753 / 41563\n",
            "754 / 41563\n",
            "755 / 41563\n",
            "756 / 41563\n",
            "757 / 41563\n",
            "758 / 41563\n",
            "759 / 41563\n",
            "760 / 41563\n",
            "761 / 41563\n",
            "762 / 41563\n",
            "763 / 41563\n",
            "764 / 41563\n",
            "765 / 41563\n",
            "766 / 41563\n",
            "767 / 41563\n",
            "768 / 41563\n",
            "769 / 41563\n",
            "770 / 41563\n",
            "771 / 41563\n",
            "772 / 41563\n",
            "773 / 41563\n",
            "774 / 41563\n",
            "775 / 41563\n",
            "776 / 41563\n",
            "777 / 41563\n",
            "778 / 41563\n",
            "779 / 41563\n",
            "780 / 41563\n",
            "781 / 41563\n",
            "782 / 41563\n",
            "783 / 41563\n",
            "784 / 41563\n",
            "785 / 41563\n",
            "786 / 41563\n",
            "787 / 41563\n",
            "788 / 41563\n",
            "789 / 41563\n",
            "790 / 41563\n",
            "791 / 41563\n",
            "792 / 41563\n",
            "793 / 41563\n",
            "794 / 41563\n",
            "795 / 41563\n",
            "796 / 41563\n",
            "797 / 41563\n",
            "798 / 41563\n",
            "799 / 41563\n",
            "800 / 41563\n",
            "801 / 41563\n",
            "802 / 41563\n",
            "803 / 41563\n",
            "804 / 41563\n",
            "805 / 41563\n",
            "806 / 41563\n",
            "807 / 41563\n",
            "808 / 41563\n",
            "809 / 41563\n",
            "810 / 41563\n",
            "811 / 41563\n",
            "812 / 41563\n",
            "813 / 41563\n",
            "814 / 41563\n",
            "815 / 41563\n",
            "816 / 41563\n",
            "817 / 41563\n",
            "818 / 41563\n",
            "819 / 41563\n",
            "820 / 41563\n",
            "821 / 41563\n",
            "822 / 41563\n",
            "823 / 41563\n",
            "824 / 41563\n",
            "825 / 41563\n",
            "826 / 41563\n",
            "827 / 41563\n",
            "828 / 41563\n",
            "829 / 41563\n",
            "830 / 41563\n",
            "831 / 41563\n",
            "832 / 41563\n",
            "833 / 41563\n",
            "834 / 41563\n",
            "835 / 41563\n",
            "836 / 41563\n",
            "837 / 41563\n",
            "838 / 41563\n",
            "839 / 41563\n",
            "840 / 41563\n",
            "841 / 41563\n",
            "842 / 41563\n",
            "843 / 41563\n",
            "844 / 41563\n",
            "845 / 41563\n",
            "846 / 41563\n",
            "847 / 41563\n",
            "848 / 41563\n",
            "849 / 41563\n",
            "850 / 41563\n",
            "851 / 41563\n",
            "852 / 41563\n",
            "853 / 41563\n",
            "854 / 41563\n",
            "855 / 41563\n",
            "856 / 41563\n",
            "857 / 41563\n",
            "858 / 41563\n",
            "859 / 41563\n",
            "860 / 41563\n",
            "861 / 41563\n",
            "862 / 41563\n",
            "863 / 41563\n",
            "864 / 41563\n",
            "865 / 41563\n",
            "866 / 41563\n",
            "867 / 41563\n",
            "868 / 41563\n",
            "869 / 41563\n",
            "870 / 41563\n",
            "871 / 41563\n",
            "872 / 41563\n",
            "873 / 41563\n",
            "874 / 41563\n",
            "875 / 41563\n",
            "876 / 41563\n",
            "877 / 41563\n",
            "878 / 41563\n",
            "879 / 41563\n",
            "880 / 41563\n",
            "881 / 41563\n",
            "882 / 41563\n",
            "883 / 41563\n",
            "884 / 41563\n",
            "885 / 41563\n",
            "886 / 41563\n",
            "887 / 41563\n",
            "888 / 41563\n",
            "889 / 41563\n",
            "890 / 41563\n",
            "891 / 41563\n",
            "892 / 41563\n",
            "893 / 41563\n",
            "894 / 41563\n",
            "895 / 41563\n",
            "896 / 41563\n",
            "897 / 41563\n",
            "898 / 41563\n",
            "899 / 41563\n",
            "900 / 41563\n",
            "901 / 41563\n",
            "902 / 41563\n",
            "903 / 41563\n",
            "904 / 41563\n",
            "905 / 41563\n",
            "906 / 41563\n",
            "907 / 41563\n",
            "908 / 41563\n",
            "909 / 41563\n",
            "910 / 41563\n",
            "911 / 41563\n",
            "912 / 41563\n",
            "913 / 41563\n",
            "914 / 41563\n",
            "915 / 41563\n",
            "916 / 41563\n",
            "917 / 41563\n",
            "918 / 41563\n",
            "919 / 41563\n",
            "920 / 41563\n",
            "921 / 41563\n",
            "922 / 41563\n",
            "923 / 41563\n",
            "924 / 41563\n",
            "925 / 41563\n",
            "926 / 41563\n",
            "927 / 41563\n",
            "928 / 41563\n",
            "929 / 41563\n",
            "930 / 41563\n",
            "931 / 41563\n",
            "932 / 41563\n",
            "933 / 41563\n",
            "934 / 41563\n",
            "935 / 41563\n",
            "936 / 41563\n",
            "937 / 41563\n",
            "938 / 41563\n",
            "939 / 41563\n",
            "940 / 41563\n",
            "941 / 41563\n",
            "942 / 41563\n",
            "943 / 41563\n",
            "944 / 41563\n",
            "945 / 41563\n",
            "946 / 41563\n",
            "947 / 41563\n",
            "948 / 41563\n",
            "949 / 41563\n",
            "950 / 41563\n",
            "951 / 41563\n",
            "952 / 41563\n",
            "953 / 41563\n",
            "954 / 41563\n",
            "955 / 41563\n",
            "956 / 41563\n",
            "957 / 41563\n",
            "958 / 41563\n",
            "959 / 41563\n",
            "960 / 41563\n",
            "961 / 41563\n",
            "962 / 41563\n",
            "963 / 41563\n",
            "964 / 41563\n",
            "965 / 41563\n",
            "966 / 41563\n",
            "967 / 41563\n",
            "968 / 41563\n",
            "969 / 41563\n",
            "970 / 41563\n",
            "971 / 41563\n",
            "972 / 41563\n",
            "973 / 41563\n",
            "974 / 41563\n",
            "975 / 41563\n",
            "976 / 41563\n",
            "977 / 41563\n",
            "978 / 41563\n",
            "979 / 41563\n",
            "980 / 41563\n",
            "981 / 41563\n",
            "982 / 41563\n",
            "983 / 41563\n",
            "984 / 41563\n",
            "985 / 41563\n",
            "986 / 41563\n",
            "987 / 41563\n",
            "988 / 41563\n",
            "989 / 41563\n",
            "990 / 41563\n",
            "991 / 41563\n",
            "992 / 41563\n",
            "993 / 41563\n",
            "994 / 41563\n",
            "995 / 41563\n",
            "996 / 41563\n",
            "997 / 41563\n",
            "998 / 41563\n",
            "999 / 41563\n"
          ]
        }
      ],
      "source": [
        "data = get_raw_data_go()\n",
        "D = get_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l0pIy6D8Rrf"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "bacth_size = 256\n",
        "def train():\n",
        "    criterion = nn.L1Loss()\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    length = len(D)\n",
        "    for step in range(10):\n",
        "        i = 0\n",
        "        step_loss = None\n",
        "        while i < length: \n",
        "            optimizer.zero_grad()\n",
        "            total_loss = None\n",
        "            for b in range(bacth_size):\n",
        "                if i + b == length:\n",
        "                    break\n",
        "                (d, p, v) = D[i + b]\n",
        "                p_model, v_model = model(d.clone().detach().requires_grad_(True))\n",
        "                loss = criterion(p_model, p)\n",
        "                #loss /= (Board._BOARDSIZE ** 2 + 1)\n",
        "                loss += torch.abs(v - v_model[0])\n",
        "                if total_loss is None:\n",
        "                    total_loss = loss\n",
        "                else:\n",
        "                    total_loss += loss\n",
        "            total_loss /= b\n",
        "            if step_loss is None:\n",
        "                step_loss = total_loss\n",
        "            else:\n",
        "                step_loss += total_loss\n",
        "                \n",
        "            print(\"Loss:\", float(total_loss))\n",
        "            total_loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            i += bacth_size\n",
        "        print(\"Step\", step,\":\", float(step_loss))\n",
        "        torch.save(model.state_dict(), 'gdrive/MyDrive/AlphaGo/model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S5tOpHC8VAt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d8d49c-4e68-4ff9-a222-9f9c789ac3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23192870616912842\n",
            "Loss: 0.2323043942451477\n",
            "Loss: 0.23811018466949463\n",
            "Loss: 0.2646026611328125\n",
            "Loss: 0.24451427161693573\n",
            "Loss: 0.21633054316043854\n",
            "Loss: 0.23105086386203766\n",
            "Loss: 0.2534175217151642\n",
            "Loss: 0.2808460295200348\n",
            "Loss: 0.23603731393814087\n",
            "Loss: 0.2899099886417389\n",
            "Loss: 0.23955701291561127\n",
            "Loss: 0.24349907040596008\n",
            "Loss: 0.23081044852733612\n",
            "Loss: 0.22311532497406006\n",
            "Loss: 0.24443155527114868\n",
            "Loss: 0.25064459443092346\n",
            "Loss: 0.2865793704986572\n",
            "Loss: 0.333417147397995\n",
            "Loss: 0.280410498380661\n",
            "Loss: 0.2461094707250595\n",
            "Loss: 0.2737206816673279\n",
            "Loss: 0.2988211214542389\n",
            "Loss: 0.24837444722652435\n",
            "Loss: 0.2426285743713379\n",
            "Loss: 0.23345813155174255\n",
            "Loss: 0.25874635577201843\n",
            "Loss: 0.2621980905532837\n",
            "Loss: 0.2598097622394562\n",
            "Step 0 : 7.375383377075195\n",
            "Loss: 0.24084357917308807\n",
            "Loss: 0.22547674179077148\n",
            "Loss: 0.23460838198661804\n",
            "Loss: 0.24632249772548676\n",
            "Loss: 0.23803475499153137\n",
            "Loss: 0.21576648950576782\n",
            "Loss: 0.24666233360767365\n",
            "Loss: 0.25787752866744995\n",
            "Loss: 0.2743490934371948\n",
            "Loss: 0.24159212410449982\n",
            "Loss: 0.27940109372138977\n",
            "Loss: 0.2564956247806549\n",
            "Loss: 0.2358373999595642\n",
            "Loss: 0.22058698534965515\n",
            "Loss: 0.19444556534290314\n",
            "Loss: 0.23275816440582275\n",
            "Loss: 0.23998616635799408\n",
            "Loss: 0.2780366837978363\n",
            "Loss: 0.33715325593948364\n",
            "Loss: 0.2713175117969513\n",
            "Loss: 0.22663044929504395\n",
            "Loss: 0.26477691531181335\n",
            "Loss: 0.2888036072254181\n",
            "Loss: 0.23944209516048431\n",
            "Loss: 0.21653589606285095\n",
            "Loss: 0.21799573302268982\n",
            "Loss: 0.25567781925201416\n",
            "Loss: 0.26317188143730164\n",
            "Loss: 0.24793441593647003\n",
            "Step 1 : 7.1885199546813965\n",
            "Loss: 0.22732797265052795\n",
            "Loss: 0.22531631588935852\n",
            "Loss: 0.2432520091533661\n",
            "Loss: 0.2497064769268036\n",
            "Loss: 0.23940977454185486\n",
            "Loss: 0.21949592232704163\n",
            "Loss: 0.26606371998786926\n",
            "Loss: 0.2564793825149536\n",
            "Loss: 0.27172839641571045\n",
            "Loss: 0.23435690999031067\n",
            "Loss: 0.2740033268928528\n",
            "Loss: 0.228772833943367\n",
            "Loss: 0.2354830503463745\n",
            "Loss: 0.22460931539535522\n",
            "Loss: 0.21382014453411102\n",
            "Loss: 0.26077109575271606\n",
            "Loss: 0.22656865417957306\n",
            "Loss: 0.28450697660446167\n",
            "Loss: 0.3233487606048584\n",
            "Loss: 0.26482319831848145\n",
            "Loss: 0.2347383052110672\n",
            "Loss: 0.2545453906059265\n",
            "Loss: 0.283279150724411\n",
            "Loss: 0.25045084953308105\n",
            "Loss: 0.22753870487213135\n",
            "Loss: 0.24958905577659607\n",
            "Loss: 0.25792184472084045\n",
            "Loss: 0.2623071074485779\n",
            "Loss: 0.22605271637439728\n",
            "Step 2 : 7.21626615524292\n",
            "Loss: 0.22795230150222778\n",
            "Loss: 0.21691186726093292\n",
            "Loss: 0.23269252479076385\n",
            "Loss: 0.2428593784570694\n",
            "Loss: 0.22569873929023743\n",
            "Loss: 0.2211073935031891\n",
            "Loss: 0.26347148418426514\n",
            "Loss: 0.24321113526821136\n",
            "Loss: 0.2669616639614105\n",
            "Loss: 0.2382509708404541\n",
            "Loss: 0.2675957977771759\n",
            "Loss: 0.22069977223873138\n",
            "Loss: 0.24452431499958038\n",
            "Loss: 0.22353418171405792\n",
            "Loss: 0.2101358324289322\n",
            "Loss: 0.24674488604068756\n",
            "Loss: 0.2414952665567398\n",
            "Loss: 0.27652499079704285\n",
            "Loss: 0.33008453249931335\n",
            "Loss: 0.2742770314216614\n",
            "Loss: 0.22427651286125183\n",
            "Loss: 0.25173574686050415\n",
            "Loss: 0.28214457631111145\n",
            "Loss: 0.25643131136894226\n",
            "Loss: 0.22215348482131958\n",
            "Loss: 0.21222074329853058\n",
            "Loss: 0.24372726678848267\n",
            "Loss: 0.25409969687461853\n",
            "Loss: 0.2218160629272461\n",
            "Step 3 : 7.083339214324951\n",
            "Loss: 0.23317502439022064\n",
            "Loss: 0.21740303933620453\n",
            "Loss: 0.21999132633209229\n",
            "Loss: 0.2528083920478821\n",
            "Loss: 0.2269189953804016\n",
            "Loss: 0.25238916277885437\n",
            "Loss: 0.25631582736968994\n",
            "Loss: 0.2295198142528534\n",
            "Loss: 0.2727701961994171\n",
            "Loss: 0.2444625198841095\n",
            "Loss: 0.277168869972229\n",
            "Loss: 0.2268686145544052\n",
            "Loss: 0.22885467112064362\n",
            "Loss: 0.20174378156661987\n",
            "Loss: 0.20336852967739105\n",
            "Loss: 0.2355659157037735\n",
            "Loss: 0.23343265056610107\n",
            "Loss: 0.2835400402545929\n",
            "Loss: 0.3346843719482422\n",
            "Loss: 0.24883447587490082\n",
            "Loss: 0.23802410066127777\n",
            "Loss: 0.24476736783981323\n",
            "Loss: 0.27133676409721375\n",
            "Loss: 0.2508854568004608\n",
            "Loss: 0.23122857511043549\n",
            "Loss: 0.23576384782791138\n",
            "Loss: 0.25766265392303467\n",
            "Loss: 0.2643274962902069\n",
            "Loss: 0.2555060088634491\n",
            "Step 4 : 7.129318714141846\n",
            "Loss: 0.23328740894794464\n",
            "Loss: 0.22279495000839233\n",
            "Loss: 0.21147772669792175\n",
            "Loss: 0.23584865033626556\n",
            "Loss: 0.24091742932796478\n",
            "Loss: 0.2238646149635315\n",
            "Loss: 0.25751596689224243\n",
            "Loss: 0.24594876170158386\n",
            "Loss: 0.2577446401119232\n",
            "Loss: 0.215549036860466\n",
            "Loss: 0.2573421597480774\n",
            "Loss: 0.2281848043203354\n",
            "Loss: 0.2191227376461029\n",
            "Loss: 0.2320595383644104\n",
            "Loss: 0.2018619030714035\n",
            "Loss: 0.2487078458070755\n",
            "Loss: 0.2243390679359436\n",
            "Loss: 0.2782309651374817\n",
            "Loss: 0.32891646027565\n",
            "Loss: 0.2656210660934448\n",
            "Loss: 0.23549631237983704\n",
            "Loss: 0.24658800661563873\n",
            "Loss: 0.28997883200645447\n",
            "Loss: 0.2427176982164383\n",
            "Loss: 0.2177606076002121\n",
            "Loss: 0.22750698029994965\n",
            "Loss: 0.23818854987621307\n",
            "Loss: 0.26197296380996704\n",
            "Loss: 0.23470717668533325\n",
            "Step 5 : 7.0242533683776855\n",
            "Loss: 0.23300080001354218\n",
            "Loss: 0.21031521260738373\n",
            "Loss: 0.211909681558609\n",
            "Loss: 0.24031628668308258\n",
            "Loss: 0.23541229963302612\n",
            "Loss: 0.2242964655160904\n",
            "Loss: 0.2645350992679596\n",
            "Loss: 0.2421799749135971\n",
            "Loss: 0.2827361822128296\n",
            "Loss: 0.2443484514951706\n",
            "Loss: 0.26674532890319824\n",
            "Loss: 0.23373010754585266\n",
            "Loss: 0.22111114859580994\n",
            "Loss: 0.2167232185602188\n",
            "Loss: 0.197966530919075\n",
            "Loss: 0.23702125251293182\n",
            "Loss: 0.23581334948539734\n",
            "Loss: 0.26232582330703735\n",
            "Loss: 0.3119547963142395\n",
            "Loss: 0.2560168206691742\n",
            "Loss: 0.2257506549358368\n",
            "Loss: 0.2513476610183716\n",
            "Loss: 0.2902490496635437\n",
            "Loss: 0.23808442056179047\n",
            "Loss: 0.2171529233455658\n",
            "Loss: 0.22636477649211884\n",
            "Loss: 0.23635070025920868\n",
            "Loss: 0.2551698684692383\n",
            "Loss: 0.2051500380039215\n",
            "Step 6 : 6.974078178405762\n",
            "Loss: 0.23287400603294373\n",
            "Loss: 0.213053360581398\n",
            "Loss: 0.2083449810743332\n",
            "Loss: 0.23674315214157104\n",
            "Loss: 0.24015578627586365\n",
            "Loss: 0.22826838493347168\n",
            "Loss: 0.25384005904197693\n",
            "Loss: 0.23584182560443878\n",
            "Loss: 0.26821169257164\n",
            "Loss: 0.2327151596546173\n",
            "Loss: 0.2420169711112976\n",
            "Loss: 0.2354952096939087\n",
            "Loss: 0.2160012423992157\n",
            "Loss: 0.21356700360774994\n",
            "Loss: 0.2111949920654297\n",
            "Loss: 0.24219021201133728\n",
            "Loss: 0.2334258258342743\n",
            "Loss: 0.26215100288391113\n",
            "Loss: 0.3088918924331665\n",
            "Loss: 0.2674415111541748\n",
            "Loss: 0.23563037812709808\n",
            "Loss: 0.23066024482250214\n",
            "Loss: 0.2769750654697418\n",
            "Loss: 0.22776123881340027\n",
            "Loss: 0.19760441780090332\n",
            "Loss: 0.21479257941246033\n",
            "Loss: 0.240481898188591\n",
            "Loss: 0.2576896846294403\n",
            "Loss: 0.19107598066329956\n",
            "Step 7 : 6.855095386505127\n",
            "Loss: 0.22884754836559296\n",
            "Loss: 0.19638392329216003\n",
            "Loss: 0.2174759805202484\n",
            "Loss: 0.2529635727405548\n",
            "Loss: 0.20786690711975098\n",
            "Loss: 0.2109721153974533\n",
            "Loss: 0.24365578591823578\n",
            "Loss: 0.24297446012496948\n",
            "Loss: 0.2613409459590912\n",
            "Loss: 0.22954662144184113\n",
            "Loss: 0.2654178738594055\n",
            "Loss: 0.2394973486661911\n",
            "Loss: 0.22285693883895874\n",
            "Loss: 0.2242804616689682\n",
            "Loss: 0.2049100399017334\n",
            "Loss: 0.2260553538799286\n",
            "Loss: 0.2260519117116928\n",
            "Loss: 0.28041166067123413\n",
            "Loss: 0.3210691511631012\n",
            "Loss: 0.2664015293121338\n",
            "Loss: 0.22453898191452026\n",
            "Loss: 0.2514275908470154\n",
            "Loss: 0.2580547630786896\n",
            "Loss: 0.2471158653497696\n",
            "Loss: 0.20759081840515137\n",
            "Loss: 0.1996665894985199\n",
            "Loss: 0.23729802668094635\n",
            "Loss: 0.2409428507089615\n",
            "Loss: 0.2125633955001831\n",
            "Step 8 : 6.848179340362549\n",
            "Loss: 0.24257603287696838\n",
            "Loss: 0.20502308011054993\n",
            "Loss: 0.20877769589424133\n",
            "Loss: 0.21543686091899872\n",
            "Loss: 0.21565166115760803\n",
            "Loss: 0.22547608613967896\n",
            "Loss: 0.246329665184021\n",
            "Loss: 0.24898108839988708\n",
            "Loss: 0.2565328776836395\n",
            "Loss: 0.2368621528148651\n",
            "Loss: 0.2773579955101013\n",
            "Loss: 0.2326175719499588\n",
            "Loss: 0.20921385288238525\n",
            "Loss: 0.20869113504886627\n",
            "Loss: 0.2058396339416504\n",
            "Loss: 0.23181436955928802\n",
            "Loss: 0.2358853965997696\n",
            "Loss: 0.26940664649009705\n",
            "Loss: 0.31771233677864075\n",
            "Loss: 0.24419930577278137\n",
            "Loss: 0.21878549456596375\n",
            "Loss: 0.24136917293071747\n",
            "Loss: 0.2466038465499878\n",
            "Loss: 0.2254250943660736\n",
            "Loss: 0.20029085874557495\n",
            "Loss: 0.22533917427062988\n",
            "Loss: 0.23542378842830658\n",
            "Loss: 0.2588791847229004\n",
            "Loss: 0.21172545850276947\n",
            "Step 9 : 6.798227310180664\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('gdrive/MyDrive/AlphaGo/model_weights.pth'))\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}